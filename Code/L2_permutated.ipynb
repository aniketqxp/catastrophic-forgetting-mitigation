{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d28e2c6-8381-4a12-b4f4-0115063fc09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79567d42-ac0f-4662-883f-112bdff611ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Permuted MNIST Dataset\n",
    "class PermutedMNIST(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, permutations=None):\n",
    "        self.mnist_dataset = torchvision.datasets.MNIST(root=root, train=train, transform=transforms.ToTensor(), download=True)\n",
    "        self.transform = transform\n",
    "        self.permutations = permutations\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.mnist_dataset[idx]\n",
    "        if self.permutations is not None:\n",
    "            image = image.view(-1)[self.permutations].view(image.shape)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Setup Permuted MNIST Tasks\n",
    "num_tasks = 5\n",
    "input_size = 28 * 28  # Flattened MNIST image\n",
    "permutations = [torch.randperm(input_size) for _ in range(num_tasks)]\n",
    "\n",
    "# Load Permuted MNIST Datasets for each task\n",
    "train_tasks = [PermutedMNIST(root=\"./data\", train=True, permutations=permutations[i]) for i in range(num_tasks)]\n",
    "test_tasks = [PermutedMNIST(root=\"./data\", train=False, permutations=permutations[i]) for i in range(num_tasks)]\n",
    "\n",
    "# Function to create DataLoaders for each task\n",
    "def get_task_data(task_idx, batch_size=64):\n",
    "    train_loader = DataLoader(train_tasks[task_idx], batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_tasks[task_idx], batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e9f78b1-3887-4914-a6f2-6de6946e1ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing anchor lambda = 0.001\n",
      "\n",
      "Trying L2 lambda = 0.001000\n",
      "Task 1, Epoch 1/1, Loss: 1.2138, Accuracy: 76.25%\n",
      "Task 2, Epoch 1/1, Loss: 0.8167, Accuracy: 80.67%\n",
      "Task 3, Epoch 1/1, Loss: 0.7203, Accuracy: 81.53%\n",
      "Task 4, Epoch 1/1, Loss: 0.6726, Accuracy: 82.43%\n",
      "Task 5, Epoch 1/1, Loss: 0.6704, Accuracy: 82.16%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 78.15%\n",
      "Task 2 Accuracy: 81.90%\n",
      "Task 3 Accuracy: 82.32%\n",
      "Task 4 Accuracy: 86.70%\n",
      "Task 5 Accuracy: 89.14%\n",
      "Lambda 0.001000 | Task 1 Final Accuracy: 78.15%\n",
      "\n",
      "üîç Testing anchor lambda = 0.01\n",
      "\n",
      "Trying L2 lambda = 0.010000\n",
      "Task 1, Epoch 1/1, Loss: 1.2275, Accuracy: 74.39%\n",
      "Task 2, Epoch 1/1, Loss: 0.8334, Accuracy: 81.39%\n",
      "Task 3, Epoch 1/1, Loss: 0.8145, Accuracy: 81.33%\n",
      "Task 4, Epoch 1/1, Loss: 0.8515, Accuracy: 82.31%\n",
      "Task 5, Epoch 1/1, Loss: 0.9242, Accuracy: 81.95%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 83.78%\n",
      "Task 2 Accuracy: 82.38%\n",
      "Task 3 Accuracy: 81.99%\n",
      "Task 4 Accuracy: 83.26%\n",
      "Task 5 Accuracy: 88.41%\n",
      "Lambda 0.010000 | Task 1 Final Accuracy: 83.78%\n",
      "\n",
      "üîç Testing anchor lambda = 0.05\n",
      "\n",
      "Trying L2 lambda = 0.050000\n",
      "Task 1, Epoch 1/1, Loss: 1.2216, Accuracy: 75.27%\n",
      "Task 2, Epoch 1/1, Loss: 0.9583, Accuracy: 80.55%\n",
      "Task 3, Epoch 1/1, Loss: 1.1167, Accuracy: 80.64%\n",
      "Task 4, Epoch 1/1, Loss: 1.2748, Accuracy: 81.16%\n",
      "Task 5, Epoch 1/1, Loss: 1.4176, Accuracy: 80.55%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 86.08%\n",
      "Task 2 Accuracy: 82.17%\n",
      "Task 3 Accuracy: 63.56%\n",
      "Task 4 Accuracy: 57.86%\n",
      "Task 5 Accuracy: 86.10%\n",
      "Lambda 0.050000 | Task 1 Final Accuracy: 86.08%\n",
      "\n",
      "üîç Testing anchor lambda = 0.1\n",
      "\n",
      "Trying L2 lambda = 0.100000\n",
      "Task 1, Epoch 1/1, Loss: 1.2052, Accuracy: 75.25%\n",
      "Task 2, Epoch 1/1, Loss: 1.0739, Accuracy: 80.06%\n",
      "Task 3, Epoch 1/1, Loss: 1.3404, Accuracy: 79.86%\n",
      "Task 4, Epoch 1/1, Loss: 1.5592, Accuracy: 78.98%\n",
      "Task 5, Epoch 1/1, Loss: 1.7284, Accuracy: 77.65%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 85.70%\n",
      "Task 2 Accuracy: 77.60%\n",
      "Task 3 Accuracy: 50.79%\n",
      "Task 4 Accuracy: 33.96%\n",
      "Task 5 Accuracy: 82.38%\n",
      "Lambda 0.100000 | Task 1 Final Accuracy: 85.70%\n",
      "\n",
      "üîç Testing anchor lambda = 1.0\n",
      "\n",
      "Trying L2 lambda = 1.000000\n",
      "Task 1, Epoch 1/1, Loss: 1.2029, Accuracy: 74.65%\n",
      "Task 2, Epoch 1/1, Loss: 1.7562, Accuracy: 69.97%\n",
      "Task 3, Epoch 1/1, Loss: 2.1751, Accuracy: 55.68%\n",
      "Task 4, Epoch 1/1, Loss: 2.3308, Accuracy: 44.86%\n",
      "Task 5, Epoch 1/1, Loss: 2.4902, Accuracy: 39.68%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 85.57%\n",
      "Task 2 Accuracy: 60.13%\n",
      "Task 3 Accuracy: 21.62%\n",
      "Task 4 Accuracy: 16.84%\n",
      "Task 5 Accuracy: 42.22%\n",
      "Lambda 1.000000 | Task 1 Final Accuracy: 85.57%\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Trying L2 lambda = 0.960981\n",
      "Task 1, Epoch 1/1, Loss: 1.2322, Accuracy: 75.58%\n",
      "Task 2, Epoch 1/1, Loss: 1.7283, Accuracy: 70.76%\n",
      "Task 3, Epoch 1/1, Loss: 2.1645, Accuracy: 57.86%\n",
      "Task 4, Epoch 1/1, Loss: 2.3061, Accuracy: 44.52%\n",
      "Task 5, Epoch 1/1, Loss: 2.4087, Accuracy: 43.42%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 85.40%\n",
      "Task 2 Accuracy: 58.76%\n",
      "Task 3 Accuracy: 23.84%\n",
      "Task 4 Accuracy: 17.03%\n",
      "Task 5 Accuracy: 44.72%\n",
      "Lambda 0.960981 | Task 1 Final Accuracy: 85.40%\n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 339.0450\n",
      "Function value obtained: -85.4000\n",
      "Current minimum: -85.4000\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Trying L2 lambda = 0.000826\n",
      "Task 1, Epoch 1/1, Loss: 1.2281, Accuracy: 74.29%\n",
      "Task 2, Epoch 1/1, Loss: 0.8070, Accuracy: 81.06%\n",
      "Task 3, Epoch 1/1, Loss: 0.7117, Accuracy: 81.62%\n",
      "Task 4, Epoch 1/1, Loss: 0.6797, Accuracy: 82.11%\n",
      "Task 5, Epoch 1/1, Loss: 0.6670, Accuracy: 82.09%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 78.82%\n",
      "Task 2 Accuracy: 81.30%\n",
      "Task 3 Accuracy: 82.10%\n",
      "Task 4 Accuracy: 85.74%\n",
      "Task 5 Accuracy: 89.06%\n",
      "Lambda 0.000826 | Task 1 Final Accuracy: 78.82%\n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 328.5371\n",
      "Function value obtained: -78.8200\n",
      "Current minimum: -85.4000\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Trying L2 lambda = 0.791507\n",
      "Task 1, Epoch 1/1, Loss: 1.2259, Accuracy: 76.00%\n",
      "Task 2, Epoch 1/1, Loss: 1.6859, Accuracy: 73.69%\n",
      "Task 3, Epoch 1/1, Loss: 2.1144, Accuracy: 61.21%\n",
      "Task 4, Epoch 1/1, Loss: 2.3253, Accuracy: 51.12%\n",
      "Task 5, Epoch 1/1, Loss: 2.4246, Accuracy: 48.91%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 85.02%\n",
      "Task 2 Accuracy: 63.48%\n",
      "Task 3 Accuracy: 26.73%\n",
      "Task 4 Accuracy: 16.29%\n",
      "Task 5 Accuracy: 52.25%\n",
      "Lambda 0.791507 | Task 1 Final Accuracy: 85.02%\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 317.4326\n",
      "Function value obtained: -85.0200\n",
      "Current minimum: -85.4000\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Trying L2 lambda = 0.096439\n",
      "Task 1, Epoch 1/1, Loss: 1.2305, Accuracy: 74.36%\n",
      "Task 2, Epoch 1/1, Loss: 1.0727, Accuracy: 79.89%\n",
      "Task 3, Epoch 1/1, Loss: 1.3203, Accuracy: 79.61%\n",
      "Task 4, Epoch 1/1, Loss: 1.5236, Accuracy: 79.69%\n",
      "Task 5, Epoch 1/1, Loss: 1.7073, Accuracy: 77.40%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 85.27%\n",
      "Task 2 Accuracy: 79.93%\n",
      "Task 3 Accuracy: 59.06%\n",
      "Task 4 Accuracy: 40.75%\n",
      "Task 5 Accuracy: 82.45%\n",
      "Lambda 0.096439 | Task 1 Final Accuracy: 85.27%\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 332.4111\n",
      "Function value obtained: -85.2700\n",
      "Current minimum: -85.4000\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Trying L2 lambda = 0.016950\n",
      "Task 1, Epoch 1/1, Loss: 1.2288, Accuracy: 75.52%\n",
      "Task 2, Epoch 1/1, Loss: 0.8641, Accuracy: 81.00%\n",
      "Task 3, Epoch 1/1, Loss: 0.8809, Accuracy: 81.40%\n",
      "Task 4, Epoch 1/1, Loss: 0.9567, Accuracy: 82.06%\n",
      "Task 5, Epoch 1/1, Loss: 1.0506, Accuracy: 81.97%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 84.15%\n",
      "Task 2 Accuracy: 83.01%\n",
      "Task 3 Accuracy: 79.35%\n",
      "Task 4 Accuracy: 80.19%\n",
      "Task 5 Accuracy: 88.04%\n",
      "Lambda 0.016950 | Task 1 Final Accuracy: 84.15%\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 319.1501\n",
      "Function value obtained: -84.1500\n",
      "Current minimum: -85.4000\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Trying L2 lambda = 0.000316\n",
      "Task 1, Epoch 1/1, Loss: 1.2161, Accuracy: 74.27%\n",
      "Task 2, Epoch 1/1, Loss: 0.8064, Accuracy: 80.83%\n",
      "Task 3, Epoch 1/1, Loss: 0.7110, Accuracy: 81.37%\n",
      "Task 4, Epoch 1/1, Loss: 0.6584, Accuracy: 82.44%\n",
      "Task 5, Epoch 1/1, Loss: 0.6454, Accuracy: 82.26%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 78.51%\n",
      "Task 2 Accuracy: 81.93%\n",
      "Task 3 Accuracy: 83.01%\n",
      "Task 4 Accuracy: 86.33%\n",
      "Task 5 Accuracy: 89.04%\n",
      "Lambda 0.000316 | Task 1 Final Accuracy: 78.51%\n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 299.5114\n",
      "Function value obtained: -78.5100\n",
      "Current minimum: -85.4000\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Trying L2 lambda = 0.019781\n",
      "Task 1, Epoch 1/1, Loss: 1.2305, Accuracy: 73.91%\n",
      "Task 2, Epoch 1/1, Loss: 0.8667, Accuracy: 80.81%\n",
      "Task 3, Epoch 1/1, Loss: 0.8980, Accuracy: 81.47%\n",
      "Task 4, Epoch 1/1, Loss: 0.9975, Accuracy: 82.01%\n",
      "Task 5, Epoch 1/1, Loss: 1.0947, Accuracy: 81.75%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 84.81%\n",
      "Task 2 Accuracy: 83.29%\n",
      "Task 3 Accuracy: 79.17%\n",
      "Task 4 Accuracy: 78.40%\n",
      "Task 5 Accuracy: 88.06%\n",
      "Lambda 0.019781 | Task 1 Final Accuracy: 84.81%\n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 310.1245\n",
      "Function value obtained: -84.8100\n",
      "Current minimum: -85.4000\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Trying L2 lambda = 0.004662\n",
      "Task 1, Epoch 1/1, Loss: 1.1913, Accuracy: 76.81%\n",
      "Task 2, Epoch 1/1, Loss: 0.8103, Accuracy: 81.27%\n",
      "Task 3, Epoch 1/1, Loss: 0.7561, Accuracy: 81.68%\n",
      "Task 4, Epoch 1/1, Loss: 0.7527, Accuracy: 82.42%\n",
      "Task 5, Epoch 1/1, Loss: 0.7953, Accuracy: 82.08%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 81.69%\n",
      "Task 2 Accuracy: 83.23%\n",
      "Task 3 Accuracy: 83.26%\n",
      "Task 4 Accuracy: 85.76%\n",
      "Task 5 Accuracy: 89.04%\n",
      "Lambda 0.004662 | Task 1 Final Accuracy: 81.69%\n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 313.4685\n",
      "Function value obtained: -81.6900\n",
      "Current minimum: -85.4000\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Trying L2 lambda = 0.000518\n",
      "Task 1, Epoch 1/1, Loss: 1.2055, Accuracy: 75.32%\n",
      "Task 2, Epoch 1/1, Loss: 0.8094, Accuracy: 80.65%\n",
      "Task 3, Epoch 1/1, Loss: 0.7224, Accuracy: 81.13%\n",
      "Task 4, Epoch 1/1, Loss: 0.6632, Accuracy: 82.42%\n",
      "Task 5, Epoch 1/1, Loss: 0.6582, Accuracy: 81.87%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 79.58%\n",
      "Task 2 Accuracy: 81.63%\n",
      "Task 3 Accuracy: 82.00%\n",
      "Task 4 Accuracy: 85.68%\n",
      "Task 5 Accuracy: 88.90%\n",
      "Lambda 0.000518 | Task 1 Final Accuracy: 79.58%\n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 304.7069\n",
      "Function value obtained: -79.5800\n",
      "Current minimum: -85.4000\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Trying L2 lambda = 0.179656\n",
      "Task 1, Epoch 1/1, Loss: 1.2303, Accuracy: 75.17%\n",
      "Task 2, Epoch 1/1, Loss: 1.2253, Accuracy: 78.75%\n",
      "Task 3, Epoch 1/1, Loss: 1.5811, Accuracy: 77.46%\n",
      "Task 4, Epoch 1/1, Loss: 1.7927, Accuracy: 75.61%\n",
      "Task 5, Epoch 1/1, Loss: 2.0044, Accuracy: 72.83%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 85.80%\n",
      "Task 2 Accuracy: 76.60%\n",
      "Task 3 Accuracy: 47.40%\n",
      "Task 4 Accuracy: 33.32%\n",
      "Task 5 Accuracy: 78.26%\n",
      "Lambda 0.179656 | Task 1 Final Accuracy: 85.80%\n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 320.3498\n",
      "Function value obtained: -85.8000\n",
      "Current minimum: -85.8000\n",
      "\n",
      "‚ö†Ô∏è Bayesian optimization underperformed manual anchors. Defaulting to best manual result.\n",
      "\n",
      "‚úÖ Best L2 lambda found: 0.050000 with Task 1 final accuracy: 86.08%\n",
      "Optimal L2 lambda value for Task 1 performance: 0.05\n"
     ]
    }
   ],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "import numpy as np\n",
    "\n",
    "def find_best_l2_lambda_task1_focus(\n",
    "    model_class, input_size, hidden_size, output_size,\n",
    "    num_tasks=5, epochs_per_task=1, n_calls=15, initial_trials=[0.001, 0.01, 0.1, 1.0]\n",
    "):\n",
    "    \"\"\"\n",
    "    Optimized L2 lambda tuner focused solely on Task 1 final performance after Task 5.\n",
    "    Includes exploration safeguards and defaults to a reliable manual value if needed.\n",
    "    \"\"\"\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "\n",
    "    # Define the search space (log scale - note different range from EWC)\n",
    "    search_space = [Real(0.0001, 10.0, \"log-uniform\", name=\"lambda\")]\n",
    "\n",
    "    # Track best lambda and performance so far\n",
    "    best_lambda = None\n",
    "    best_task1_performance = 0.0\n",
    "\n",
    "    # Objective function: Tracks only Task 1 performance at the end\n",
    "    def objective_function(params):\n",
    "        nonlocal best_lambda, best_task1_performance\n",
    "        current_lambda = params[0]\n",
    "        print(f\"\\nTrying L2 lambda = {current_lambda:.6f}\")\n",
    "\n",
    "        # Initialize a new model\n",
    "        model = model_class(input_size, hidden_size, output_size)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "        # Storage for initial parameters\n",
    "        initial_params_list = []\n",
    "\n",
    "        # Train on each task sequentially\n",
    "        for task_idx in range(num_tasks):\n",
    "            # Train on current task using L2 regularization\n",
    "            task_loss, task_acc = train_task(\n",
    "                model,\n",
    "                task_idx,\n",
    "                criterion,\n",
    "                optimizer,\n",
    "                initial_params_list=initial_params_list if task_idx > 0 else None,\n",
    "                l2_lambda=current_lambda,\n",
    "                epochs=epochs_per_task\n",
    "            )\n",
    "\n",
    "            # Store initial parameters after training\n",
    "            initial_params = store_initial_params(model)\n",
    "            initial_params_list.append(initial_params)\n",
    "\n",
    "        # Evaluate on all tasks\n",
    "        print(\"\\nEvaluating on all tasks:\")\n",
    "        task_accuracies = evaluate_all_tasks(model, num_tasks)\n",
    "\n",
    "        # Extract only Task 1 performance at the end\n",
    "        task1_performance = task_accuracies[0]\n",
    "        print(f\"Lambda {current_lambda:.6f} | Task 1 Final Accuracy: {task1_performance:.2f}%\")\n",
    "\n",
    "        # Track the best lambda value specifically for Task 1\n",
    "        if task1_performance > best_task1_performance:\n",
    "            best_task1_performance = task1_performance\n",
    "            best_lambda = current_lambda\n",
    "\n",
    "        # Return **negative Task 1 performance** for minimization\n",
    "        return -task1_performance\n",
    "\n",
    "    # Pre-run manual \"anchor\" lambdas to avoid small-value traps\n",
    "    for initial_lambda in initial_trials:\n",
    "        print(f\"\\nüîç Testing anchor lambda = {initial_lambda}\")\n",
    "        objective_function([initial_lambda])\n",
    "\n",
    "    # Run Bayesian optimization (now with good anchors)\n",
    "    result = gp_minimize(\n",
    "        objective_function,\n",
    "        search_space,\n",
    "        n_calls=n_calls,\n",
    "        random_state=42,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # If Bayesian result fails to outperform manual values, default to the manual best\n",
    "    if -result.fun < best_task1_performance:\n",
    "        print(\"\\n‚ö†Ô∏è Bayesian optimization underperformed manual anchors. Defaulting to best manual result.\")\n",
    "    else:\n",
    "        best_lambda = result.x[0]\n",
    "        best_task1_performance = -result.fun\n",
    "\n",
    "    print(f\"\\n‚úÖ Best L2 lambda found: {best_lambda:.6f} with Task 1 final accuracy: {best_task1_performance:.2f}%\")\n",
    "    return best_lambda\n",
    "\n",
    "\n",
    "# Call the function to find the optimal L2 lambda\n",
    "input_size = 28 * 28\n",
    "hidden_size = 256\n",
    "output_size = 10\n",
    "\n",
    "optimal_l2_lambda = find_best_l2_lambda_task1_focus(\n",
    "    model_class=SimpleNN,\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_size,\n",
    "    num_tasks=5,\n",
    "    epochs_per_task=1,\n",
    "    n_calls=10,\n",
    "    initial_trials=[0.001, 0.01, 0.05, 0.1, 1.0]\n",
    ")\n",
    "\n",
    "print(f\"Optimal L2 lambda value for Task 1 performance: {optimal_l2_lambda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39902908-c142-47fa-b778-bef606e5ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Function to store initial parameters for L2 regularization\n",
    "def store_initial_params(model):\n",
    "    initial_params = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        initial_params[name] = param.data.clone()\n",
    "    return initial_params\n",
    "\n",
    "# Function to train the model on a specific task with L2 regularization\n",
    "def train_task(model, task_idx, criterion, optimizer, initial_params_list=None, l2_lambda=0.01, epochs=5):\n",
    "    train_loader, _ = get_task_data(task_idx)\n",
    "    \n",
    "    # For collecting metrics\n",
    "    task_train_loss = []\n",
    "    task_train_acc = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Add L2 penalty if not the first task\n",
    "            if initial_params_list and task_idx > 0:\n",
    "                l2_loss = 0\n",
    "                for name, param in model.named_parameters():\n",
    "                    # Apply L2 regularization to keep parameters close to their initial values\n",
    "                    for init_params in initial_params_list:\n",
    "                        l2_loss += ((param - init_params[name]).pow(2)).sum()\n",
    "                \n",
    "                # Scale the L2 loss by lambda and add to the task loss\n",
    "                loss += (l2_lambda / 2) * l2_loss\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        \n",
    "        task_train_loss.append(epoch_loss)\n",
    "        task_train_acc.append(epoch_acc)\n",
    "        \n",
    "        print(f'Task {task_idx+1}, Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "    \n",
    "    return task_train_loss, task_train_acc\n",
    "\n",
    "# Function to evaluate the model on all seen tasks\n",
    "def evaluate_all_tasks(model, num_tasks):\n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(num_tasks):\n",
    "        _, test_loader = get_task_data(i)\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        accuracies.append(accuracy)\n",
    "        print(f'Task {i+1} Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "# Function to calculate forgetting metrics\n",
    "def calculate_forgetting_metrics(training_history, initial_accuracies):\n",
    "    forgetting_rate = {}\n",
    "    \n",
    "    # For each task (except the last one since we don't have measurements after it)\n",
    "    for task_idx in range(len(initial_accuracies) - 1):\n",
    "        forgetting = []\n",
    "        \n",
    "        # Calculate forgetting for the task at each subsequent evaluation point\n",
    "        for eval_idx, accuracies in enumerate(training_history[\"task_accuracies\"]):\n",
    "            if task_idx <= eval_idx:  # We only have measurements for tasks we've seen\n",
    "                forgetting.append(initial_accuracies[task_idx] - accuracies[task_idx])\n",
    "        \n",
    "        forgetting_rate[f\"Task {task_idx+1}\"] = forgetting\n",
    "    \n",
    "    return forgetting_rate\n",
    "\n",
    "# Main function to demonstrate L2 regularization for mitigating catastrophic forgetting\n",
    "def demonstrate_l2_regularization():\n",
    "    # Hyperparameters\n",
    "    input_size = 28 * 28  # Flattened MNIST image\n",
    "    hidden_size = 256\n",
    "    output_size = 10  # 10 classes for Permuted MNIST\n",
    "    learning_rate = 0.01\n",
    "    epochs_per_task = 5\n",
    "    l2_lambda = 0.01  # L2 regularization strength\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SimpleNN(input_size, hidden_size, output_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Use SGD without weight decay (we'll implement L2 regularization manually)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Print model configuration\n",
    "    print(f\"Model Configuration:\")\n",
    "    print(f\"- SGD with L2 Regularization (lambda={l2_lambda})\")\n",
    "    print(f\"- Learning Rate: {learning_rate}\")\n",
    "    print(f\"- Hidden Size: {hidden_size}\")\n",
    "    print(f\"- Epochs per Task: {epochs_per_task}\")\n",
    "    \n",
    "    # To store metrics\n",
    "    training_history = {\n",
    "        \"task_accuracies\": [],  # Performance on each task after sequential training\n",
    "        \"training_time\": [],    # Time taken to train each task\n",
    "        \"learning_curves\": {    # Loss and accuracy during training\n",
    "            \"loss\": [],\n",
    "            \"accuracy\": []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # To compute forgetting metrics\n",
    "    initial_accuracies = []  # Accuracy on each task right after learning it\n",
    "    \n",
    "    # Store initial parameters for each task\n",
    "    initial_params_list = []\n",
    "    \n",
    "    # Train on each task sequentially\n",
    "    for task_idx in range(len(train_tasks)):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training on Task {task_idx+1}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Measure training time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train on current task using L2 regularization if not the first task\n",
    "        task_loss, task_acc = train_task(\n",
    "            model, \n",
    "            task_idx, \n",
    "            criterion, \n",
    "            optimizer, \n",
    "            initial_params_list=initial_params_list if task_idx > 0 else None,\n",
    "            l2_lambda=l2_lambda,\n",
    "            epochs=epochs_per_task\n",
    "        )\n",
    "        \n",
    "        # Record training time\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        training_history[\"training_time\"].append(training_time)\n",
    "        \n",
    "        # Save learning curves\n",
    "        training_history[\"learning_curves\"][\"loss\"].extend(task_loss)\n",
    "        training_history[\"learning_curves\"][\"accuracy\"].extend(task_acc)\n",
    "        \n",
    "        # After training on this task, store the current parameters\n",
    "        initial_params = store_initial_params(model)\n",
    "        initial_params_list.append(initial_params)\n",
    "        \n",
    "        # Evaluate on all tasks seen so far\n",
    "        print(\"\\nEvaluating on all tasks seen so far:\")\n",
    "        task_accuracies = evaluate_all_tasks(model, task_idx + 1)\n",
    "        \n",
    "        # Store the accuracy on the current task after learning it\n",
    "        if task_idx == 0:\n",
    "            initial_accuracies.append(task_accuracies[0])\n",
    "        else:\n",
    "            training_history[\"task_accuracies\"].append(task_accuracies.copy())\n",
    "            initial_accuracies.append(task_accuracies[task_idx])\n",
    "    \n",
    "    # Calculate forgetting metrics\n",
    "    forgetting_rate = calculate_forgetting_metrics(training_history, initial_accuracies)\n",
    "    \n",
    "    return training_history, forgetting_rate, initial_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44efd47a-911b-47df-93be-77096edfc36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configuration:\n",
      "- SGD with L2 Regularization (lambda=0.01)\n",
      "- Learning Rate: 0.01\n",
      "- Hidden Size: 256\n",
      "- Epochs per Task: 5\n",
      "\n",
      "==================================================\n",
      "Training on Task 1\n",
      "==================================================\n",
      "Task 1, Epoch 1/5, Loss: 1.2179, Accuracy: 75.57%\n",
      "Task 1, Epoch 2/5, Loss: 0.4913, Accuracy: 87.52%\n",
      "Task 1, Epoch 3/5, Loss: 0.3914, Accuracy: 89.28%\n",
      "Task 1, Epoch 4/5, Loss: 0.3502, Accuracy: 90.19%\n",
      "Task 1, Epoch 5/5, Loss: 0.3253, Accuracy: 90.81%\n",
      "\n",
      "Evaluating on all tasks seen so far:\n",
      "Task 1 Accuracy: 91.59%\n",
      "\n",
      "==================================================\n",
      "Training on Task 2\n",
      "==================================================\n",
      "Task 2, Epoch 1/5, Loss: 0.7003, Accuracy: 82.60%\n",
      "Task 2, Epoch 2/5, Loss: 0.4436, Accuracy: 89.43%\n",
      "Task 2, Epoch 3/5, Loss: 0.4124, Accuracy: 90.39%\n",
      "Task 2, Epoch 4/5, Loss: 0.3981, Accuracy: 91.02%\n",
      "Task 2, Epoch 5/5, Loss: 0.3888, Accuracy: 91.41%\n",
      "\n",
      "Evaluating on all tasks seen so far:\n",
      "Task 1 Accuracy: 89.95%\n",
      "Task 2 Accuracy: 92.12%\n",
      "\n",
      "==================================================\n",
      "Training on Task 3\n",
      "==================================================\n",
      "Task 3, Epoch 1/5, Loss: 0.7540, Accuracy: 81.88%\n",
      "Task 3, Epoch 2/5, Loss: 0.5327, Accuracy: 89.46%\n",
      "Task 3, Epoch 3/5, Loss: 0.5084, Accuracy: 90.50%\n",
      "Task 3, Epoch 4/5, Loss: 0.4969, Accuracy: 91.01%\n",
      "Task 3, Epoch 5/5, Loss: 0.4896, Accuracy: 91.31%\n",
      "\n",
      "Evaluating on all tasks seen so far:\n",
      "Task 1 Accuracy: 88.84%\n",
      "Task 2 Accuracy: 87.35%\n",
      "Task 3 Accuracy: 91.93%\n",
      "\n",
      "==================================================\n",
      "Training on Task 4\n",
      "==================================================\n",
      "Task 4, Epoch 1/5, Loss: 0.7964, Accuracy: 83.50%\n",
      "Task 4, Epoch 2/5, Loss: 0.6099, Accuracy: 89.57%\n",
      "Task 4, Epoch 3/5, Loss: 0.5852, Accuracy: 90.33%\n",
      "Task 4, Epoch 4/5, Loss: 0.5733, Accuracy: 90.76%\n",
      "Task 4, Epoch 5/5, Loss: 0.5668, Accuracy: 90.95%\n",
      "\n",
      "Evaluating on all tasks seen so far:\n",
      "Task 1 Accuracy: 89.48%\n",
      "Task 2 Accuracy: 80.42%\n",
      "Task 3 Accuracy: 75.39%\n",
      "Task 4 Accuracy: 91.27%\n",
      "\n",
      "==================================================\n",
      "Training on Task 5\n",
      "==================================================\n",
      "Task 5, Epoch 1/5, Loss: 0.8798, Accuracy: 82.89%\n",
      "Task 5, Epoch 2/5, Loss: 0.6884, Accuracy: 89.19%\n",
      "Task 5, Epoch 3/5, Loss: 0.6639, Accuracy: 89.94%\n",
      "Task 5, Epoch 4/5, Loss: 0.6537, Accuracy: 90.32%\n",
      "Task 5, Epoch 5/5, Loss: 0.6492, Accuracy: 90.45%\n",
      "\n",
      "Evaluating on all tasks seen so far:\n",
      "Task 1 Accuracy: 88.80%\n",
      "Task 2 Accuracy: 81.77%\n",
      "Task 3 Accuracy: 60.30%\n",
      "Task 4 Accuracy: 51.58%\n",
      "Task 5 Accuracy: 90.77%\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    training_history, forgetting_rate, initial_accuracies = demonstrate_l2_regularization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d883b72-de18-4c5d-8f0e-56201de397ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
