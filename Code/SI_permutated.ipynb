{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "350b8e42-a0bc-40e0-a923-8c36433f51a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4e0c6eb-02ea-488e-af4c-954b31b1af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Permuted MNIST Dataset\n",
    "class PermutedMNIST(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, permutations=None):\n",
    "        self.mnist_dataset = torchvision.datasets.MNIST(root=root, train=train, transform=transforms.ToTensor(), download=True)\n",
    "        self.transform = transform\n",
    "        self.permutations = permutations\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.mnist_dataset[idx]\n",
    "        if self.permutations is not None:\n",
    "            image = image.view(-1)[self.permutations].view(image.shape)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Setup Permuted MNIST Tasks\n",
    "num_tasks = 5\n",
    "input_size = 28 * 28  # Flattened MNIST image\n",
    "permutations = [torch.randperm(input_size) for _ in range(num_tasks)]\n",
    "\n",
    "# Load Permuted MNIST Datasets for each task\n",
    "train_tasks = [PermutedMNIST(root=\"./data\", train=True, permutations=permutations[i]) for i in range(num_tasks)]\n",
    "test_tasks = [PermutedMNIST(root=\"./data\", train=False, permutations=permutations[i]) for i in range(num_tasks)]\n",
    "\n",
    "# Function to create DataLoaders for each task\n",
    "def get_task_data(task_idx, batch_size=64):\n",
    "    train_loader = DataLoader(train_tasks[task_idx], batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_tasks[task_idx], batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "041c607f-734c-43c0-bcfa-668889aa9674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Trying SI lambda = 24.53\n",
      "Task 1, Epoch 1/1, Loss: 1.1940, Accuracy: 75.44%\n",
      "Task 2, Epoch 1/1, Loss: 0.7927, Accuracy: 81.73%\n",
      "Task 3, Epoch 1/1, Loss: 0.6902, Accuracy: 82.47%\n",
      "Task 4, Epoch 1/1, Loss: 0.6393, Accuracy: 82.72%\n",
      "Task 5, Epoch 1/1, Loss: 0.6498, Accuracy: 81.56%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 76.60%\n",
      "Task 2 Accuracy: 84.84%\n",
      "Task 3 Accuracy: 83.86%\n",
      "Task 4 Accuracy: 87.05%\n",
      "Task 5 Accuracy: 88.91%\n",
      "Lambda 24.53 | Task 1 Final Accuracy: 76.60%\n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 164.0642\n",
      "Function value obtained: -76.6000\n",
      "Current minimum: -76.6000\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Trying SI lambda = 0.36\n",
      "Task 1, Epoch 1/1, Loss: 1.2074, Accuracy: 74.91%\n",
      "Task 2, Epoch 1/1, Loss: 0.7836, Accuracy: 82.21%\n",
      "Task 3, Epoch 1/1, Loss: 0.6922, Accuracy: 82.33%\n",
      "Task 4, Epoch 1/1, Loss: 0.6410, Accuracy: 82.78%\n",
      "Task 5, Epoch 1/1, Loss: 0.6426, Accuracy: 81.58%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 76.09%\n",
      "Task 2 Accuracy: 83.77%\n",
      "Task 3 Accuracy: 84.16%\n",
      "Task 4 Accuracy: 86.26%\n",
      "Task 5 Accuracy: 89.01%\n",
      "Lambda 0.36 | Task 1 Final Accuracy: 76.09%\n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 171.5259\n",
      "Function value obtained: -76.0900\n",
      "Current minimum: -76.6000\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Trying SI lambda = 21.83\n",
      "Task 1, Epoch 1/1, Loss: 1.2119, Accuracy: 74.57%\n",
      "Task 2, Epoch 1/1, Loss: 0.7956, Accuracy: 81.65%\n",
      "Task 3, Epoch 1/1, Loss: 0.6915, Accuracy: 82.49%\n",
      "Task 4, Epoch 1/1, Loss: 0.6361, Accuracy: 83.00%\n",
      "Task 5, Epoch 1/1, Loss: 0.6499, Accuracy: 81.58%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 77.93%\n",
      "Task 2 Accuracy: 85.63%\n",
      "Task 3 Accuracy: 83.85%\n",
      "Task 4 Accuracy: 87.67%\n",
      "Task 5 Accuracy: 88.74%\n",
      "Lambda 21.83 | Task 1 Final Accuracy: 77.93%\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 178.7650\n",
      "Function value obtained: -77.9300\n",
      "Current minimum: -77.9300\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Trying SI lambda = 6.17\n",
      "Task 1, Epoch 1/1, Loss: 1.1967, Accuracy: 75.14%\n",
      "Task 2, Epoch 1/1, Loss: 0.7980, Accuracy: 81.83%\n",
      "Task 3, Epoch 1/1, Loss: 0.6862, Accuracy: 82.55%\n",
      "Task 4, Epoch 1/1, Loss: 0.6467, Accuracy: 82.56%\n",
      "Task 5, Epoch 1/1, Loss: 0.6484, Accuracy: 81.44%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 75.88%\n",
      "Task 2 Accuracy: 85.57%\n",
      "Task 3 Accuracy: 84.58%\n",
      "Task 4 Accuracy: 87.10%\n",
      "Task 5 Accuracy: 88.91%\n",
      "Lambda 6.17 | Task 1 Final Accuracy: 75.88%\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 174.6748\n",
      "Function value obtained: -75.8800\n",
      "Current minimum: -77.9300\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Trying SI lambda = 2.18\n",
      "Task 1, Epoch 1/1, Loss: 1.2113, Accuracy: 75.80%\n",
      "Task 2, Epoch 1/1, Loss: 0.7942, Accuracy: 81.39%\n",
      "Task 3, Epoch 1/1, Loss: 0.6844, Accuracy: 82.80%\n",
      "Task 4, Epoch 1/1, Loss: 0.6346, Accuracy: 82.95%\n",
      "Task 5, Epoch 1/1, Loss: 0.6490, Accuracy: 81.44%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 76.30%\n",
      "Task 2 Accuracy: 85.33%\n",
      "Task 3 Accuracy: 84.07%\n",
      "Task 4 Accuracy: 87.06%\n",
      "Task 5 Accuracy: 88.89%\n",
      "Lambda 2.18 | Task 1 Final Accuracy: 76.30%\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 175.5503\n",
      "Function value obtained: -76.3000\n",
      "Current minimum: -77.9300\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Trying SI lambda = 0.20\n",
      "Task 1, Epoch 1/1, Loss: 1.2300, Accuracy: 74.96%\n",
      "Task 2, Epoch 1/1, Loss: 0.7902, Accuracy: 81.81%\n",
      "Task 3, Epoch 1/1, Loss: 0.6925, Accuracy: 82.27%\n",
      "Task 4, Epoch 1/1, Loss: 0.6447, Accuracy: 82.60%\n",
      "Task 5, Epoch 1/1, Loss: 0.6481, Accuracy: 81.39%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 78.57%\n",
      "Task 2 Accuracy: 85.65%\n",
      "Task 3 Accuracy: 84.54%\n",
      "Task 4 Accuracy: 87.38%\n",
      "Task 5 Accuracy: 88.89%\n",
      "Lambda 0.20 | Task 1 Final Accuracy: 78.57%\n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 175.3965\n",
      "Function value obtained: -78.5700\n",
      "Current minimum: -78.5700\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Trying SI lambda = 2.39\n",
      "Task 1, Epoch 1/1, Loss: 1.2259, Accuracy: 75.30%\n",
      "Task 2, Epoch 1/1, Loss: 0.7933, Accuracy: 81.75%\n",
      "Task 3, Epoch 1/1, Loss: 0.6941, Accuracy: 82.54%\n",
      "Task 4, Epoch 1/1, Loss: 0.6451, Accuracy: 82.47%\n",
      "Task 5, Epoch 1/1, Loss: 0.6524, Accuracy: 81.23%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 76.49%\n",
      "Task 2 Accuracy: 85.35%\n",
      "Task 3 Accuracy: 84.41%\n",
      "Task 4 Accuracy: 87.32%\n",
      "Task 5 Accuracy: 88.96%\n",
      "Lambda 2.39 | Task 1 Final Accuracy: 76.49%\n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 177.3541\n",
      "Function value obtained: -76.4900\n",
      "Current minimum: -78.5700\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Trying SI lambda = 1.00\n",
      "Task 1, Epoch 1/1, Loss: 1.2157, Accuracy: 75.83%\n",
      "Task 2, Epoch 1/1, Loss: 0.7946, Accuracy: 81.58%\n",
      "Task 3, Epoch 1/1, Loss: 0.6983, Accuracy: 82.33%\n",
      "Task 4, Epoch 1/1, Loss: 0.6470, Accuracy: 82.61%\n",
      "Task 5, Epoch 1/1, Loss: 0.6511, Accuracy: 81.39%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 77.99%\n",
      "Task 2 Accuracy: 84.93%\n",
      "Task 3 Accuracy: 84.11%\n",
      "Task 4 Accuracy: 86.88%\n",
      "Task 5 Accuracy: 89.07%\n",
      "Lambda 1.00 | Task 1 Final Accuracy: 77.99%\n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 168.3373\n",
      "Function value obtained: -77.9900\n",
      "Current minimum: -78.5700\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Trying SI lambda = 0.27\n",
      "Task 1, Epoch 1/1, Loss: 1.2153, Accuracy: 74.17%\n",
      "Task 2, Epoch 1/1, Loss: 0.8002, Accuracy: 81.41%\n",
      "Task 3, Epoch 1/1, Loss: 0.6956, Accuracy: 82.14%\n",
      "Task 4, Epoch 1/1, Loss: 0.6410, Accuracy: 82.76%\n",
      "Task 5, Epoch 1/1, Loss: 0.6438, Accuracy: 81.62%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 77.17%\n",
      "Task 2 Accuracy: 85.13%\n",
      "Task 3 Accuracy: 84.16%\n",
      "Task 4 Accuracy: 86.98%\n",
      "Task 5 Accuracy: 88.91%\n",
      "Lambda 0.27 | Task 1 Final Accuracy: 77.17%\n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 168.1182\n",
      "Function value obtained: -77.1700\n",
      "Current minimum: -78.5700\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Trying SI lambda = 8.97\n",
      "Task 1, Epoch 1/1, Loss: 1.2375, Accuracy: 74.67%\n",
      "Task 2, Epoch 1/1, Loss: 0.7972, Accuracy: 81.64%\n",
      "Task 3, Epoch 1/1, Loss: 0.6922, Accuracy: 82.65%\n",
      "Task 4, Epoch 1/1, Loss: 0.6419, Accuracy: 82.70%\n",
      "Task 5, Epoch 1/1, Loss: 0.6507, Accuracy: 81.39%\n",
      "\n",
      "Evaluating on all tasks:\n",
      "Task 1 Accuracy: 77.39%\n",
      "Task 2 Accuracy: 85.76%\n",
      "Task 3 Accuracy: 83.51%\n",
      "Task 4 Accuracy: 86.89%\n",
      "Task 5 Accuracy: 89.06%\n",
      "Lambda 8.97 | Task 1 Final Accuracy: 77.39%\n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 173.0590\n",
      "Function value obtained: -77.3900\n",
      "Current minimum: -78.5700\n",
      "\n",
      "Best SI lambda found: 0.20 with Task 1 final accuracy: 78.57%\n",
      "Optimal SI lambda value for Task 1 performance: 0.19949166150633935\n"
     ]
    }
   ],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "import numpy as np\n",
    "\n",
    "def find_optimal_si_lambda_task1_focus(\n",
    "    model_class, input_size, hidden_size, output_size,\n",
    "    num_tasks=5, epochs_per_task=1, n_calls=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Find an optimal lambda value for Synaptic Intelligence using Bayesian optimization,\n",
    "    focused exclusively on maximizing Task 1 performance after Task 5.\n",
    "    \"\"\"\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "\n",
    "    # Define the search space (log scale)\n",
    "    search_space = [Real(0.1, 100.0, \"log-uniform\", name=\"lambda\")]\n",
    "\n",
    "    # Objective function: Only cares about Task 1 performance at the very end\n",
    "    def objective_function(params):\n",
    "        current_lambda = params[0]\n",
    "        print(f\"\\nTrying SI lambda = {current_lambda:.2f}\")\n",
    "\n",
    "        # Initialize a new model\n",
    "        model = model_class(input_size, hidden_size, output_size)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "        # Storage for SI parameters\n",
    "        omega_sum = None\n",
    "        initial_params = {}\n",
    "\n",
    "        # Save initial model parameters\n",
    "        for name, param in model.named_parameters():\n",
    "            initial_params[name] = param.data.clone()\n",
    "\n",
    "        # Train on each task sequentially\n",
    "        for task_idx in range(num_tasks):\n",
    "            # Train on current task using SI\n",
    "            task_loss, task_acc, omega_curr = train_task(\n",
    "                model,\n",
    "                task_idx,\n",
    "                criterion,\n",
    "                optimizer,\n",
    "                omega_sum=omega_sum,\n",
    "                initial_params=initial_params if task_idx > 0 else None,\n",
    "                si_lambda=current_lambda,\n",
    "                epochs=epochs_per_task\n",
    "            )\n",
    "\n",
    "            # Update accumulated importance weights (omega_sum)\n",
    "            if omega_sum is None:\n",
    "                omega_sum = omega_curr\n",
    "            else:\n",
    "                for name in omega_sum:\n",
    "                    omega_sum[name] += omega_curr[name]\n",
    "\n",
    "            # Save new initial parameters for the next task\n",
    "            for name, param in model.named_parameters():\n",
    "                initial_params[name] = param.data.clone()\n",
    "\n",
    "        # Evaluate on all tasks\n",
    "        print(\"\\nEvaluating on all tasks:\")\n",
    "        task_accuracies = evaluate_all_tasks(model, num_tasks)\n",
    "\n",
    "        # Extract **only Task 1 accuracy** at the end\n",
    "        task1_performance = task_accuracies[0]\n",
    "        print(f\"Lambda {current_lambda:.2f} | Task 1 Final Accuracy: {task1_performance:.2f}%\")\n",
    "\n",
    "        # Return **negative Task 1 performance** for minimization\n",
    "        return -task1_performance\n",
    "\n",
    "    # Run Bayesian optimization\n",
    "    result = gp_minimize(\n",
    "        objective_function,\n",
    "        search_space,\n",
    "        n_calls=n_calls,\n",
    "        random_state=42,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Get the best lambda specifically for Task 1 performance\n",
    "    best_lambda = result.x[0]\n",
    "    best_performance = -result.fun\n",
    "\n",
    "    print(f\"\\nBest SI lambda found: {best_lambda:.2f} with Task 1 final accuracy: {best_performance:.2f}%\")\n",
    "    return best_lambda\n",
    "\n",
    "\n",
    "# Call the function to find the optimal SI lambda\n",
    "input_size = 28 * 28\n",
    "hidden_size = 256\n",
    "output_size = 10\n",
    "\n",
    "optimal_si_lambda = find_optimal_si_lambda_task1_focus(\n",
    "    model_class=SimpleNN,\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_size,\n",
    "    num_tasks=5,\n",
    "    epochs_per_task=1,  # Keep it efficient\n",
    "    n_calls=10  # Minimum search effort\n",
    ")\n",
    "\n",
    "print(f\"Optimal SI lambda value for Task 1 performance: {optimal_si_lambda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e248c32-7da3-4c8f-9be8-0ae5e9c2c7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Function to initialize synaptic intelligence trackers\n",
    "def initialize_si_trackers(model):\n",
    "    # Initialize parameter importance (omega)\n",
    "    omega = {}\n",
    "    # Initialize parameter change accumulator (path integral)\n",
    "    path_integral = {}\n",
    "    # Initialize old parameter values\n",
    "    old_params = {}\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        omega[name] = torch.zeros_like(param.data)\n",
    "        path_integral[name] = torch.zeros_like(param.data)\n",
    "        old_params[name] = param.data.clone()\n",
    "    \n",
    "    return omega, path_integral, old_params\n",
    "\n",
    "# Function to update synaptic intelligence trackers\n",
    "def update_si_trackers(model, path_integral, old_params):\n",
    "    for name, param in model.named_parameters():\n",
    "        # Compute parameter change\n",
    "        delta = param.data - old_params[name]\n",
    "        # Update path integral with parameter change * gradient\n",
    "        if param.grad is not None:\n",
    "            path_integral[name] += -param.grad * delta\n",
    "        # Update old parameter values\n",
    "        old_params[name] = param.data.clone()\n",
    "    \n",
    "    return path_integral, old_params\n",
    "\n",
    "# Function to compute the synaptic intelligence omega (importance)\n",
    "def compute_omega(model, path_integral, old_params, epsilon=0.1):\n",
    "    omega_new = {}\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        # Compute parameter change\n",
    "        delta = param.data - old_params[name]\n",
    "        # Compute importance (omega) based on path integral and parameter change\n",
    "        # Add epsilon to avoid division by zero\n",
    "        delta_norm = torch.norm(delta)\n",
    "        if delta_norm > 0:\n",
    "            omega_new[name] = path_integral[name] / (delta.pow(2) + epsilon)\n",
    "        else:\n",
    "            omega_new[name] = torch.zeros_like(param.data)\n",
    "    \n",
    "    return omega_new\n",
    "\n",
    "# Function to compute the SI penalty\n",
    "def si_penalty(model, omega_sum, initial_params):\n",
    "    penalty = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        # Compute squared parameter change from old value\n",
    "        delta = (param.data - initial_params[name]).pow(2)\n",
    "        # Compute the penalty based on importance and parameter change\n",
    "        penalty += (omega_sum[name] * delta).sum()\n",
    "    \n",
    "    return penalty\n",
    "\n",
    "# Function to train the model on a specific task with Synaptic Intelligence\n",
    "def train_task(model, task_idx, criterion, optimizer, omega_sum=None, initial_params=None, si_lambda=1.0, epochs=5):\n",
    "    train_loader, _ = get_task_data(task_idx)\n",
    "    \n",
    "    # Initialize SI trackers for current task\n",
    "    omega_curr, path_integral, old_params = initialize_si_trackers(model)\n",
    "    \n",
    "    # For collecting metrics\n",
    "    task_train_loss = []\n",
    "    task_train_acc = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Add SI penalty if not the first task\n",
    "            if omega_sum is not None and initial_params is not None:\n",
    "                si_loss = si_penalty(model, omega_sum, initial_params)\n",
    "                loss += si_lambda * si_loss\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update SI trackers\n",
    "            path_integral, old_params = update_si_trackers(model, path_integral, old_params)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        \n",
    "        task_train_loss.append(epoch_loss)\n",
    "        task_train_acc.append(epoch_acc)\n",
    "        \n",
    "        print(f'Task {task_idx+1}, Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "    \n",
    "    # Compute importance (omega) after training on this task\n",
    "    omega_curr = compute_omega(model, path_integral, old_params)\n",
    "    \n",
    "    return task_train_loss, task_train_acc, omega_curr\n",
    "\n",
    "# Function to evaluate the model on all seen tasks\n",
    "def evaluate_all_tasks(model, num_tasks):\n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(num_tasks):\n",
    "        _, test_loader = get_task_data(i)\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        accuracies.append(accuracy)\n",
    "        print(f'Task {i+1} Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "# Function to calculate forgetting metrics\n",
    "def calculate_forgetting_metrics(training_history, initial_accuracies):\n",
    "    forgetting_rate = {}\n",
    "    \n",
    "    # For each task (except the last one since we don't have measurements after it)\n",
    "    for task_idx in range(len(initial_accuracies) - 1):\n",
    "        forgetting = []\n",
    "        \n",
    "        # Calculate forgetting for the task at each subsequent evaluation point\n",
    "        for eval_idx, accuracies in enumerate(training_history[\"task_accuracies\"]):\n",
    "            if task_idx <= eval_idx:  # We only have measurements for tasks we've seen\n",
    "                forgetting.append(initial_accuracies[task_idx] - accuracies[task_idx])\n",
    "        \n",
    "        forgetting_rate[f\"Task {task_idx+1}\"] = forgetting\n",
    "    \n",
    "    return forgetting_rate\n",
    "\n",
    "# Main function to demonstrate Synaptic Intelligence for mitigating catastrophic forgetting\n",
    "def demonstrate_synaptic_intelligence():\n",
    "    # Hyperparameters\n",
    "    input_size = 28 * 28  # Flattened MNIST image\n",
    "    hidden_size = 256\n",
    "    output_size = 10  # 10 classes for Permuted MNIST\n",
    "    learning_rate = 0.01\n",
    "    epochs_per_task = 5\n",
    "    si_lambda = 0.2  # Synaptic Intelligence regularization strength\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SimpleNN(input_size, hidden_size, output_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Use SGD without weight decay (SI replaces L2 regularization)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Print model configuration\n",
    "    print(f\"Model Configuration:\")\n",
    "    print(f\"- SGD with Synaptic Intelligence (lambda={si_lambda})\")\n",
    "    print(f\"- Learning Rate: {learning_rate}\")\n",
    "    print(f\"- Hidden Size: {hidden_size}\")\n",
    "    print(f\"- Epochs per Task: {epochs_per_task}\")\n",
    "    \n",
    "    # To store metrics\n",
    "    training_history = {\n",
    "        \"task_accuracies\": [],  # Performance on each task after sequential training\n",
    "        \"training_time\": [],    # Time taken to train each task\n",
    "        \"learning_curves\": {    # Loss and accuracy during training\n",
    "            \"loss\": [],\n",
    "            \"accuracy\": []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # To compute forgetting metrics\n",
    "    initial_accuracies = []  # Accuracy on each task right after learning it\n",
    "    \n",
    "    # Store accumulated importance weights (omega) and initial parameters\n",
    "    omega_sum = None\n",
    "    initial_params = {}\n",
    "    \n",
    "    # Save initial model parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        initial_params[name] = param.data.clone()\n",
    "    \n",
    "    # Train on each task sequentially\n",
    "    for task_idx in range(len(train_tasks)): # Change split_mnist_tasks to train_tasks\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training on Task {task_idx+1}\") #Remove task details from print\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Measure training time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train on current task using SI if not the first task\n",
    "        task_loss, task_acc, omega_curr = train_task(\n",
    "            model, \n",
    "            task_idx, \n",
    "            criterion, \n",
    "            optimizer, \n",
    "            omega_sum=omega_sum,\n",
    "            initial_params=initial_params if task_idx > 0 else None,\n",
    "            si_lambda=si_lambda,\n",
    "            epochs=epochs_per_task\n",
    "        )\n",
    "        \n",
    "        # Record training time\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        training_history[\"training_time\"].append(training_time)\n",
    "        \n",
    "        # Save learning curves\n",
    "        training_history[\"learning_curves\"][\"loss\"].extend(task_loss)\n",
    "        training_history[\"learning_curves\"][\"accuracy\"].extend(task_acc)\n",
    "        \n",
    "        # Update accumulated importance weights (omega_sum)\n",
    "        if omega_sum is None:\n",
    "            omega_sum = omega_curr\n",
    "        else:\n",
    "            for name in omega_sum:\n",
    "                omega_sum[name] += omega_curr[name]\n",
    "        \n",
    "        # Save new initial parameters for the next task\n",
    "        for name, param in model.named_parameters():\n",
    "            initial_params[name] = param.data.clone()\n",
    "        \n",
    "        # Evaluate on all tasks seen so far\n",
    "        print(\"\\nEvaluating on all tasks seen so far:\")\n",
    "        task_accuracies = evaluate_all_tasks(model, task_idx + 1)\n",
    "\n",
    "        # Store the accuracy on the current task after learning it\n",
    "        if task_idx == 0:\n",
    "            initial_accuracies.append(task_accuracies[0])\n",
    "        else:\n",
    "            training_history[\"task_accuracies\"].append(task_accuracies.copy())\n",
    "            initial_accuracies.append(task_accuracies[task_idx])\n",
    "\n",
    "    # Calculate forgetting metrics\n",
    "    forgetting_rate = calculate_forgetting_metrics(training_history, initial_accuracies)\n",
    "\n",
    "    return training_history, forgetting_rate, initial_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3adf44db-3a73-4fc3-83f7-1a60d0a2044f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configuration:\n",
      "- SGD with Synaptic Intelligence (lambda=0.2)\n",
      "- Learning Rate: 0.01\n",
      "- Hidden Size: 256\n",
      "- Epochs per Task: 5\n",
      "\n",
      "==================================================\n",
      "Training on Task 1\n",
      "==================================================\n",
      "Task 1, Epoch 1/5, Loss: 1.2055, Accuracy: 75.65%\n",
      "Task 1, Epoch 2/5, Loss: 0.4859, Accuracy: 87.79%\n",
      "Task 1, Epoch 3/5, Loss: 0.3880, Accuracy: 89.40%\n",
      "Task 1, Epoch 4/5, Loss: 0.3469, Accuracy: 90.26%\n",
      "Task 1, Epoch 5/5, Loss: 0.3215, Accuracy: 90.90%\n",
      "\n",
      "Evaluating on all tasks seen so far:\n",
      "Task 1 Accuracy: 91.55%\n",
      "\n",
      "==================================================\n",
      "Training on Task 2\n",
      "==================================================\n",
      "Task 2, Epoch 1/5, Loss: 0.6660, Accuracy: 82.58%\n",
      "Task 2, Epoch 2/5, Loss: 0.3784, Accuracy: 89.66%\n",
      "Task 2, Epoch 3/5, Loss: 0.3303, Accuracy: 90.78%\n",
      "Task 2, Epoch 4/5, Loss: 0.3030, Accuracy: 91.44%\n",
      "Task 2, Epoch 5/5, Loss: 0.2835, Accuracy: 91.94%\n",
      "\n",
      "Evaluating on all tasks seen so far:\n",
      "Task 1 Accuracy: 87.99%\n",
      "Task 2 Accuracy: 92.50%\n",
      "\n",
      "==================================================\n",
      "Training on Task 3\n",
      "==================================================\n",
      "Task 3, Epoch 1/5, Loss: 0.5987, Accuracy: 83.19%\n",
      "Task 3, Epoch 2/5, Loss: 0.3557, Accuracy: 89.99%\n",
      "Task 3, Epoch 3/5, Loss: 0.3125, Accuracy: 91.18%\n",
      "Task 3, Epoch 4/5, Loss: 0.2867, Accuracy: 91.90%\n",
      "Task 3, Epoch 5/5, Loss: 0.2682, Accuracy: 92.37%\n",
      "\n",
      "Evaluating on all tasks seen so far:\n",
      "Task 1 Accuracy: 78.94%\n",
      "Task 2 Accuracy: 88.70%\n",
      "Task 3 Accuracy: 92.86%\n",
      "\n",
      "==================================================\n",
      "Training on Task 4\n",
      "==================================================\n",
      "Task 4, Epoch 1/5, Loss: 0.5816, Accuracy: 82.88%\n",
      "Task 4, Epoch 2/5, Loss: 0.3469, Accuracy: 90.17%\n",
      "Task 4, Epoch 3/5, Loss: 0.3042, Accuracy: 91.31%\n",
      "Task 4, Epoch 4/5, Loss: 0.2783, Accuracy: 92.09%\n",
      "Task 4, Epoch 5/5, Loss: 0.2591, Accuracy: 92.59%\n",
      "\n",
      "Evaluating on all tasks seen so far:\n",
      "Task 1 Accuracy: 71.92%\n",
      "Task 2 Accuracy: 89.34%\n",
      "Task 3 Accuracy: 90.48%\n",
      "Task 4 Accuracy: 93.01%\n",
      "\n",
      "==================================================\n",
      "Training on Task 5\n",
      "==================================================\n",
      "Task 5, Epoch 1/5, Loss: 0.6019, Accuracy: 81.89%\n",
      "Task 5, Epoch 2/5, Loss: 0.3464, Accuracy: 90.01%\n",
      "Task 5, Epoch 3/5, Loss: 0.3022, Accuracy: 91.34%\n",
      "Task 5, Epoch 4/5, Loss: 0.2753, Accuracy: 92.16%\n",
      "Task 5, Epoch 5/5, Loss: 0.2559, Accuracy: 92.64%\n",
      "\n",
      "Evaluating on all tasks seen so far:\n",
      "Task 1 Accuracy: 70.87%\n",
      "Task 2 Accuracy: 83.39%\n",
      "Task 3 Accuracy: 85.21%\n",
      "Task 4 Accuracy: 90.29%\n",
      "Task 5 Accuracy: 93.15%\n"
     ]
    }
   ],
   "source": [
    "# Example usage (you can call this function in your main script)\n",
    "if __name__ == \"__main__\":\n",
    "    training_history, forgetting_rate, initial_accuracies = demonstrate_synaptic_intelligence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a52348-0119-45ef-b856-7ba566564e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
