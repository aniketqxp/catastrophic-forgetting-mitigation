{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678077a1-b37e-4104-a26d-7384595dbe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12bd9e55-9c20-4ee7-9e64-f96c293bc07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Class Incremental MNIST Dataset\n",
    "class ClassIncrementalMNIST(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, classes=None):\n",
    "        self.mnist_dataset = torchvision.datasets.MNIST(root=root, train=train, transform=transforms.ToTensor(), download=True)\n",
    "        self.transform = transform\n",
    "        self.classes = classes\n",
    "        self.train = train\n",
    "        # Filter data to include only the specified classes\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        for image, label in self.mnist_dataset:\n",
    "            if label in self.classes:\n",
    "                self.data.append(image)\n",
    "                self.targets.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx], self.targets[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Setup Class Incremental MNIST Tasks\n",
    "num_tasks = 5\n",
    "classes_per_task = 2\n",
    "\n",
    "# Divide classes into tasks\n",
    "class_splits = [list(range(i * classes_per_task, (i + 1) * classes_per_task)) for i in range(num_tasks)]\n",
    "\n",
    "# Load datasets for each task\n",
    "train_tasks = [ClassIncrementalMNIST(root=\"./data\", train=True, classes=class_splits[i]) for i in range(num_tasks)]\n",
    "test_tasks = [ClassIncrementalMNIST(root=\"./data\", train=False, classes=class_splits[i]) for i in range(num_tasks)]\n",
    "\n",
    "# Function to create DataLoaders\n",
    "def get_task_data(task_idx, batch_size=64):\n",
    "    train_loader = DataLoader(train_tasks[task_idx], batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_tasks[task_idx], batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4822caf5-a7a5-4b14-b900-aadea389426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SimpleNN model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Train function with naive rehearsal\n",
    "def train_task_with_replay(model, task_idx, criterion, optimizer, replay_data, replay_labels, epochs=5):\n",
    "    train_loader, _ = get_task_data(task_idx)\n",
    "    \n",
    "    # Combine current task data with replay data\n",
    "    combined_data = torch.utils.data.TensorDataset(\n",
    "        torch.cat([torch.stack([x for x, _ in train_loader.dataset]), replay_data]),\n",
    "        torch.cat([torch.tensor([y for _, y in train_loader.dataset], dtype=torch.long), replay_labels])\n",
    "    )\n",
    "    combined_loader = DataLoader(combined_data, batch_size=64, shuffle=True)\n",
    "    \n",
    "    # For collecting metrics\n",
    "    task_train_loss = []\n",
    "    task_train_acc = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in combined_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(combined_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        \n",
    "        task_train_loss.append(epoch_loss)\n",
    "        task_train_acc.append(epoch_acc)\n",
    "        \n",
    "        print(f'Task {task_idx+1}, Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "    \n",
    "    return task_train_loss, task_train_acc\n",
    "\n",
    "# Evaluate function for cumulative classes\n",
    "def evaluate_cumulative_classes(model, num_tasks):\n",
    "    combined_test_set = ConcatDataset(test_tasks[:num_tasks])\n",
    "    test_loader = DataLoader(combined_test_set, batch_size=64, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Naive Rehearsal continual learning demonstration\n",
    "def demonstrate_naive_rehearsal():\n",
    "    input_size = 28 * 28\n",
    "    hidden_size = 256\n",
    "    learning_rate = 0.01\n",
    "    epochs_per_task = 5\n",
    "    \n",
    "    # Start with output size for first task\n",
    "    output_size = 10  # All MNIST digits (0-9)\n",
    "    \n",
    "    model = SimpleNN(input_size, hidden_size, output_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    accuracies = []\n",
    "    replay_data = torch.tensor([])\n",
    "    replay_labels = torch.tensor([], dtype=torch.long)  # Ensure replay_labels is of type torch.long\n",
    "    \n",
    "    for task_idx in range(len(class_splits)):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training on Task {task_idx+1}: Classes {class_splits[task_idx]}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Train on current task with replay\n",
    "        train_loss, train_acc = train_task_with_replay(model, task_idx, criterion, optimizer, replay_data, replay_labels, epochs_per_task)\n",
    "        \n",
    "        # Evaluate on cumulative classes\n",
    "        accuracy = evaluate_cumulative_classes(model, task_idx + 1)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"\\nModel Accuracy after Task {task_idx + 1}: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Store a subset of current task data for replay\n",
    "        current_task_data = torch.stack([x for x, _ in train_tasks[task_idx]])\n",
    "        current_task_labels = torch.tensor([y for _, y in train_tasks[task_idx]], dtype=torch.long)\n",
    "        \n",
    "        # Randomly select a subset of data for replay\n",
    "        replay_size = 100  # Number of samples to store for replay\n",
    "        indices = torch.randperm(len(current_task_data))[:replay_size]\n",
    "        replay_data = torch.cat([replay_data, current_task_data[indices]])\n",
    "        replay_labels = torch.cat([replay_labels, current_task_labels[indices]])\n",
    "    \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6df37ac2-dfb4-478a-ae16-e9595290f975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training on Task 1: Classes [0, 1]\n",
      "==================================================\n",
      "Task 1, Epoch 1/5, Loss: 0.4516, Accuracy: 96.98%\n",
      "Task 1, Epoch 2/5, Loss: 0.0373, Accuracy: 99.67%\n",
      "Task 1, Epoch 3/5, Loss: 0.0217, Accuracy: 99.75%\n",
      "Task 1, Epoch 4/5, Loss: 0.0162, Accuracy: 99.76%\n",
      "Task 1, Epoch 5/5, Loss: 0.0134, Accuracy: 99.77%\n",
      "\n",
      "Model Accuracy after Task 1: 99.91%\n",
      "\n",
      "==================================================\n",
      "Training on Task 2: Classes [2, 3]\n",
      "==================================================\n",
      "Task 2, Epoch 1/5, Loss: 0.7270, Accuracy: 83.19%\n",
      "Task 2, Epoch 2/5, Loss: 0.1863, Accuracy: 94.59%\n",
      "Task 2, Epoch 3/5, Loss: 0.1485, Accuracy: 95.36%\n",
      "Task 2, Epoch 4/5, Loss: 0.1327, Accuracy: 95.68%\n",
      "Task 2, Epoch 5/5, Loss: 0.1231, Accuracy: 95.93%\n",
      "\n",
      "Model Accuracy after Task 2: 52.49%\n",
      "\n",
      "==================================================\n",
      "Training on Task 3: Classes [4, 5]\n",
      "==================================================\n",
      "Task 3, Epoch 1/5, Loss: 0.7729, Accuracy: 83.04%\n",
      "Task 3, Epoch 2/5, Loss: 0.2067, Accuracy: 95.73%\n",
      "Task 3, Epoch 3/5, Loss: 0.1639, Accuracy: 96.52%\n",
      "Task 3, Epoch 4/5, Loss: 0.1411, Accuracy: 96.91%\n",
      "Task 3, Epoch 5/5, Loss: 0.1283, Accuracy: 97.16%\n",
      "\n",
      "Model Accuracy after Task 3: 43.72%\n",
      "\n",
      "==================================================\n",
      "Training on Task 4: Classes [6, 7]\n",
      "==================================================\n",
      "Task 4, Epoch 1/5, Loss: 0.5885, Accuracy: 88.55%\n",
      "Task 4, Epoch 2/5, Loss: 0.1281, Accuracy: 97.51%\n",
      "Task 4, Epoch 3/5, Loss: 0.1033, Accuracy: 97.64%\n",
      "Task 4, Epoch 4/5, Loss: 0.0918, Accuracy: 97.81%\n",
      "Task 4, Epoch 5/5, Loss: 0.0828, Accuracy: 97.95%\n",
      "\n",
      "Model Accuracy after Task 4: 45.09%\n",
      "\n",
      "==================================================\n",
      "Training on Task 5: Classes [8, 9]\n",
      "==================================================\n",
      "Task 5, Epoch 1/5, Loss: 0.6525, Accuracy: 85.58%\n",
      "Task 5, Epoch 2/5, Loss: 0.2426, Accuracy: 93.34%\n",
      "Task 5, Epoch 3/5, Loss: 0.2134, Accuracy: 93.94%\n",
      "Task 5, Epoch 4/5, Loss: 0.1967, Accuracy: 94.40%\n",
      "Task 5, Epoch 5/5, Loss: 0.1856, Accuracy: 94.68%\n",
      "\n",
      "Model Accuracy after Task 5: 32.19%\n"
     ]
    }
   ],
   "source": [
    "# Run the demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    naive_rehearsal_accuracies = demonstrate_naive_rehearsal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660ab16e-278d-4c13-8725-503fc68629ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
