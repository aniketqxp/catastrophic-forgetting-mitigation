{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458048a8-78b5-4066-9444-3a20d4c1e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0b20a9-8f9a-4132-9809-90a68fda7fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Class Incremental MNIST Dataset\n",
    "class ClassIncrementalMNIST(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, classes=None):\n",
    "        self.mnist_dataset = torchvision.datasets.MNIST(root=root, train=train, transform=transforms.ToTensor(), download=True)\n",
    "        self.transform = transform\n",
    "        self.classes = classes\n",
    "        self.train = train\n",
    "        # Filter data to include only the specified classes\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        for image, label in self.mnist_dataset:\n",
    "            if label in self.classes:\n",
    "                self.data.append(image)\n",
    "                self.targets.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx], self.targets[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Setup Class Incremental MNIST Tasks\n",
    "num_tasks = 5\n",
    "classes_per_task = 2\n",
    "\n",
    "# Divide classes into tasks\n",
    "class_splits = [list(range(i * classes_per_task, (i + 1) * classes_per_task)) for i in range(num_tasks)]\n",
    "\n",
    "# Load datasets for each task\n",
    "train_tasks = [ClassIncrementalMNIST(root=\"./data\", train=True, classes=class_splits[i]) for i in range(num_tasks)]\n",
    "test_tasks = [ClassIncrementalMNIST(root=\"./data\", train=False, classes=class_splits[i]) for i in range(num_tasks)]\n",
    "\n",
    "# Function to create DataLoaders\n",
    "def get_task_data(task_idx, batch_size=64):\n",
    "    train_loader = DataLoader(train_tasks[task_idx], batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_tasks[task_idx], batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf2186b-be12-434d-80c5-538f921bdf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Experience Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "    def add(self, images, labels):\n",
    "        self.data.extend(images)\n",
    "        self.labels.extend(labels)\n",
    "        \n",
    "        # Keep buffer size within capacity\n",
    "        if len(self.data) > self.capacity:\n",
    "            self.data = self.data[-self.capacity:]\n",
    "            self.labels = self.labels[-self.capacity:]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = random.sample(range(len(self.data)), min(batch_size, len(self.data)))\n",
    "        return torch.stack([self.data[i] for i in indices]), torch.tensor([self.labels[i] for i in indices], dtype=torch.long)\n",
    "\n",
    "# Define SimpleNN model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Train function with experience replay\n",
    "def train_with_experience_replay(model, task_idx, criterion, optimizer, buffer, epochs=5, batch_size=64):\n",
    "    train_loader, _ = get_task_data(task_idx)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            # Sample from the buffer\n",
    "            if len(buffer.data) > 0:\n",
    "                replay_inputs, replay_labels = buffer.sample(batch_size // 2)\n",
    "                inputs = torch.cat((inputs, replay_inputs))\n",
    "                labels = torch.cat((labels, replay_labels))\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        print(f'Task {task_idx+1}, Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "\n",
    "# Evaluate function for cumulative classes\n",
    "def evaluate_cumulative_classes(model, num_tasks):\n",
    "    combined_test_set = ConcatDataset(test_tasks[:num_tasks])\n",
    "    test_loader = DataLoader(combined_test_set, batch_size=64, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Full continual learning setup with experience replay\n",
    "def demonstrate_experience_replay():\n",
    "    input_size = 28 * 28\n",
    "    hidden_size = 256\n",
    "    learning_rate = 0.01\n",
    "    epochs_per_task = 5\n",
    "    replay_capacity = 500  # Buffer capacity\n",
    "\n",
    "    # Start with output size for all 10 MNIST digits\n",
    "    output_size = 10\n",
    "\n",
    "    model = SimpleNN(input_size, hidden_size, output_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    buffer = ReplayBuffer(replay_capacity)\n",
    "    accuracies = []\n",
    "    \n",
    "    for task_idx in range(len(class_splits)):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training on Task {task_idx+1}: Classes {class_splits[task_idx]}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # Train with experience replay\n",
    "        train_with_experience_replay(model, task_idx, criterion, optimizer, buffer, epochs_per_task)\n",
    "        \n",
    "        # Evaluate on cumulative tasks\n",
    "        accuracy = evaluate_cumulative_classes(model, task_idx + 1)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"\\nModel Accuracy after Task {task_idx + 1}: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Add current task data to buffer\n",
    "        current_task_data = torch.stack([x for x, _ in train_tasks[task_idx]])\n",
    "        current_task_labels = torch.tensor([y for _, y in train_tasks[task_idx]], dtype=torch.long)\n",
    "        buffer.add(current_task_data, current_task_labels)\n",
    "\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e2dd282-e743-466f-9feb-a39b4b66a35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training on Task 1: Classes [0, 1]\n",
      "==================================================\n",
      "Task 1, Epoch 1/5, Loss: 0.4762, Accuracy: 97.37%\n",
      "Task 1, Epoch 2/5, Loss: 0.0380, Accuracy: 99.67%\n",
      "Task 1, Epoch 3/5, Loss: 0.0221, Accuracy: 99.72%\n",
      "Task 1, Epoch 4/5, Loss: 0.0165, Accuracy: 99.76%\n",
      "Task 1, Epoch 5/5, Loss: 0.0136, Accuracy: 99.76%\n",
      "\n",
      "Model Accuracy after Task 1: 99.91%\n",
      "\n",
      "==================================================\n",
      "Training on Task 2: Classes [2, 3]\n",
      "==================================================\n",
      "Task 2, Epoch 1/5, Loss: 0.7913, Accuracy: 82.96%\n",
      "Task 2, Epoch 2/5, Loss: 0.2591, Accuracy: 94.27%\n",
      "Task 2, Epoch 3/5, Loss: 0.1911, Accuracy: 95.20%\n",
      "Task 2, Epoch 4/5, Loss: 0.1617, Accuracy: 95.75%\n",
      "Task 2, Epoch 5/5, Loss: 0.1458, Accuracy: 95.96%\n",
      "\n",
      "Model Accuracy after Task 2: 96.75%\n",
      "\n",
      "==================================================\n",
      "Training on Task 3: Classes [4, 5]\n",
      "==================================================\n",
      "Task 3, Epoch 1/5, Loss: 0.8979, Accuracy: 79.11%\n",
      "Task 3, Epoch 2/5, Loss: 0.3081, Accuracy: 93.87%\n",
      "Task 3, Epoch 3/5, Loss: 0.2132, Accuracy: 95.32%\n",
      "Task 3, Epoch 4/5, Loss: 0.1787, Accuracy: 95.61%\n",
      "Task 3, Epoch 5/5, Loss: 0.1514, Accuracy: 96.11%\n",
      "\n",
      "Model Accuracy after Task 3: 67.82%\n",
      "\n",
      "==================================================\n",
      "Training on Task 4: Classes [6, 7]\n",
      "==================================================\n",
      "Task 4, Epoch 1/5, Loss: 0.7720, Accuracy: 84.88%\n",
      "Task 4, Epoch 2/5, Loss: 0.2117, Accuracy: 96.43%\n",
      "Task 4, Epoch 3/5, Loss: 0.1500, Accuracy: 97.06%\n",
      "Task 4, Epoch 4/5, Loss: 0.1278, Accuracy: 97.45%\n",
      "Task 4, Epoch 5/5, Loss: 0.1066, Accuracy: 97.70%\n",
      "\n",
      "Model Accuracy after Task 4: 47.86%\n",
      "\n",
      "==================================================\n",
      "Training on Task 5: Classes [8, 9]\n",
      "==================================================\n",
      "Task 5, Epoch 1/5, Loss: 0.6819, Accuracy: 87.08%\n",
      "Task 5, Epoch 2/5, Loss: 0.2326, Accuracy: 94.46%\n",
      "Task 5, Epoch 3/5, Loss: 0.1921, Accuracy: 95.01%\n",
      "Task 5, Epoch 4/5, Loss: 0.1671, Accuracy: 95.38%\n",
      "Task 5, Epoch 5/5, Loss: 0.1496, Accuracy: 95.81%\n",
      "\n",
      "Model Accuracy after Task 5: 36.54%\n"
     ]
    }
   ],
   "source": [
    "# Run the demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    experience_replay_accuracies = demonstrate_experience_replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d631b0d4-b4be-499d-beac-63626bfcd719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
